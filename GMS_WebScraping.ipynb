{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlSEO3BMMGpzD+UxI8W6uX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GMS9github/GMS/blob/main/GMS_WebScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the Function to scrape the Url Page\n",
        "def scrape_page(url, Blog_Titles, Blog_Dates, Blog_Image_Urls, Blog_Likes_Counts):\n",
        "    response = requests.get(url, headers=headers)#Get Response from url\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')# Parse the html content using BeautifulSoup Library\n",
        "        blog_posts = soup.find_all('article', class_=lambda x: x and x.startswith('blog-item category-blog')) #Find all elements of type article that starts with 'blog-item category-blog'\n",
        "        # Iterate over blog_posts using for loop\n",
        "        for post in blog_posts:\n",
        "            # Extract title\n",
        "            title = post.find('h6') # Find header which holds the blog title\n",
        "            Blog_Titles.append(title.text.strip() if title else 'N/A')#Append the header to list called in function scrape_page\n",
        "\n",
        "            # Extract date\n",
        "            Blog_detail = post.find('div', class_='bd-item') # Find the class which holds the blog date(here it is 'bd-item')\n",
        "            # The above Class contains different icons , so comparing with the icon that holds the Blog Date(here it is 'material-design-icon-history-clock-button')\n",
        "            if Blog_detail.find('i',class_='material-design-icon-history-clock-button'):\n",
        "              date = Blog_detail.find('span')\n",
        "              Blog_Dates.append(date.text.strip() if date else 'N/A')#append the date to list called in function scrape_page\n",
        "\n",
        "            # Extract image URL\n",
        "            img_div = post.find('div', class_='img')# Find the class which holds the blog date(here it is 'img')\n",
        "            #The above image class has url in 'data-bg' element\n",
        "            img_url = img_div.find('a')['data-bg'] if img_div else 'N/A'\n",
        "            Blog_Image_Urls.append(img_url if img_url else 'N/A')#append the date to list called in function scrape_pag\n",
        "\n",
        "            # Extract likes count\n",
        "            Blog_Likes_detail = post.find('a' , class_='zilla-likes')# Find the class which holds the blog date(here it is 'zilla-like')\n",
        "            # The above Class contains different icons , so comparing with the icon that holds the Blog Date(here it is 'material-design-icon-favorite-heart-outline-button')\n",
        "            if Blog_Likes_detail.find('i',class_='material-design-icon-favorite-heart-outline-button'):\n",
        "              likes = Blog_Likes_detail.find('span')\n",
        "              Blog_Likes_Counts.append(likes.text.strip() if likes else 'N/A')#append the date to list called in function scrape_pag\n",
        "\n",
        "        #Get the url to the Next Page Which is present in Pagination Class\n",
        "        next_page_link = soup.find('a', class_='next page-numbers') # Next page-numbers sub class of Pagination class has the url to the next page\n",
        "        if next_page_link:\n",
        "            next_page_url = next_page_link.get('href')#Get the URL from the above link\n",
        "            scrape_page(next_page_url, Blog_Titles, Blog_Dates, Blog_Image_Urls, Blog_Likes_Counts)#Recursively call until the last page\n",
        "    else:\n",
        "        print(f'Failed to retrieve the webpage: {url}')\n",
        "\n",
        "# Declare URL and header\n",
        "url = 'https://rategain.com/blog'\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}   # The 'User-Agent' header is used to mimic a real browser (like Chrome) when making requests to the website\n",
        "\n",
        "# Declare lists to save the scraped members\n",
        "Blog_Titles = []\n",
        "Blog_Dates = []\n",
        "Blog_Image_Urls = []\n",
        "Blog_Likes_Counts = []\n",
        "\n",
        "scrape_page(url, Blog_Titles, Blog_Dates, Blog_Image_Urls, Blog_Likes_Counts)\n",
        "\n",
        "# Create a Dictionary from the above lists\n",
        "Excel_Data = {\n",
        "    'Blog Title': Blog_Titles,\n",
        "    'Blog Date': Blog_Dates,\n",
        "    'Blog Image URL': Blog_Image_Urls,\n",
        "    'Likes Count': Blog_Likes_Counts\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the extracted data\n",
        "df = pd.DataFrame(Excel_Data)\n",
        "# Save the data to an Excel file\n",
        "df.to_excel('Blog_Data.xlsx', index=False)\n",
        "print('Data saved to Blog_Data.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZkmSLoS4fn1",
        "outputId": "3a0ca592-349a-4692-ecf2-d808f1e67800"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to Blog_Data.xlsx\n"
          ]
        }
      ]
    }
  ]
}